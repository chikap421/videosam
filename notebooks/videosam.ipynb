{"cells":[{"cell_type":"markdown","metadata":{"id":"n_bucACXvsqK"},"source":["# 1. Libraries Installation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102547,"status":"ok","timestamp":1716402860518,"user":{"displayName":"Chika Maduabuchi","userId":"13808453024378790419"},"user_tz":240},"id":"fNGegi2_vsqM","outputId":"b3c88074-738c-4b15-b661-0996ec72ec65"},"outputs":[],"source":["!pip install git+https://github.com/facebookresearch/segment-anything.git\n","!pip install git+https://github.com/huggingface/transformers.git\n","!pip install datasets\n","!pip install monai\n","!pip install patchify\n","!pip install tifffile\n","!pip install matplotlib\n","!pip install scipy\n","!pip install seaborn\n","!pip install scikit-learn\n","!pip install peft\n","!pip install numpy\n","!pip install torch\n","!pip install h5py\n","!pip install tqdm\n","!pip install optuna"]},{"cell_type":"markdown","metadata":{"id":"ypu_TqELvsqN"},"source":["# 2. Dataset Preparation Script"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":329},"executionInfo":{"elapsed":1660,"status":"error","timestamp":1716402910323,"user":{"displayName":"Chika Maduabuchi","userId":"13808453024378790419"},"user_tz":240},"id":"ZYypyYWbM6j-","outputId":"7c13b20b-6fc1-4172-da44-889f09f13f42"},"outputs":[],"source":["import os\n","import random\n","import numpy as np\n","import tifffile\n","import matplotlib.pyplot as plt\n","from patchify import patchify\n","from datasets import Dataset as HFDataset\n","from PIL import Image\n","\n","def load_tiff_stack(image_path, mask_path):\n","    images = tifffile.imread(image_path)\n","    masks = tifffile.imread(mask_path)\n","    return images, masks\n","\n","def visualize_random_images(images, masks, num_samples=2, random_seed=42):\n","    random.seed(random_seed)\n","    random_indices = random.sample(range(images.shape[0]), num_samples)\n","    fig, axes = plt.subplots(num_samples, 2, figsize=(10, 8))\n","    for idx, ax in zip(random_indices, axes):\n","        ax[0].imshow(images[idx], cmap='gray')\n","        ax[0].set_title(f'Image {idx}')\n","        ax[0].axis('off')\n","        ax[1].imshow(masks[idx], cmap='gray')\n","        ax[1].set_title(f'Mask {idx}')\n","        ax[1].axis('off')\n","    plt.tight_layout()\n","    plt.show()\n","    return random_indices[0]\n","\n","def patchify_images_and_masks(images, masks, patch_size, step):\n","    image_patches = []\n","    mask_patches = []\n","    for img, mask in zip(images, masks):\n","        img_patches = patchify(img, (patch_size, patch_size), step=step)\n","        mask_patches_ = patchify(mask, (patch_size, patch_size), step=step)\n","\n","        img_patches = img_patches.reshape(-1, patch_size, patch_size)\n","        mask_patches_ = mask_patches_.reshape(-1, patch_size, patch_size)\n","\n","        image_patches.extend(img_patches)\n","        mask_patches.extend(mask_patches_)\n","\n","    return np.array(image_patches), np.array(mask_patches)\n","\n","def filter_non_empty_patches(images, masks):\n","    valid_indices = [i for i, mask in enumerate(masks) if mask.max() != 0]\n","    return images[valid_indices], masks[valid_indices]\n","\n","def resize_patches(images, masks, target_size):\n","    resized_images = [np.array(Image.fromarray(img).resize(target_size, Image.NEAREST)) for img in images]\n","    resized_masks = [np.array(Image.fromarray(mask).resize(target_size, Image.NEAREST)) for mask in masks]\n","    return np.array(resized_images), np.array(resized_masks)\n","\n","def normalize_masks(masks):\n","    return (masks > 0).astype(np.float32)\n","\n","def visualize_patches(image_patches, mask_patches, num_samples=2):\n","    if len(image_patches) == 0 or len(mask_patches) == 0:\n","        print(\"Image patches or mask patches array is empty. Cannot visualize.\")\n","        return\n","\n","    fig, axes = plt.subplots(num_samples, 2, figsize=(8, 4*num_samples))\n","    selected_indices = np.random.choice(range(len(image_patches)), num_samples, replace=False)\n","    for i, idx in enumerate(selected_indices):\n","        axes[i, 0].imshow(image_patches[idx], cmap='gray')\n","        axes[i, 0].set_title(f'Image Patch {idx}')\n","        axes[i, 1].imshow(mask_patches[idx], cmap='gray')\n","        axes[i, 1].set_title(f'Mask Patch {idx}')\n","    plt.tight_layout()\n","    plt.show()\n","\n","def create_dataset(images, masks):\n","    dataset_dict = {\n","        \"image\": images.tolist(),  # Convert numpy arrays to lists\n","        \"label\": masks.tolist(),  # Convert numpy arrays to lists\n","    }\n","    return HFDataset.from_dict(dataset_dict)\n","\n","def main():\n","    image_path = \"/home/ubuntu/train/train.tif\"\n","    mask_path = \"/home/ubuntu/train/train_mask.tif\"\n","    patch_size = 100\n","    step = 100\n","    target_size = (256, 256)\n","    random_seed = 42\n","\n","    images, masks = load_tiff_stack(image_path, mask_path)\n","    print(\"Image shape:\", images.shape)\n","    print(\"Mask shape:\", masks.shape)\n","\n","    selected_index = visualize_random_images(images, masks, random_seed=random_seed)\n","    print(\"Selected image index:\", selected_index)\n","\n","    positive_pixel_value = np.unique(masks)[1]\n","    print(\"Positive pixel value:\", positive_pixel_value)\n","\n","    image_patches, mask_patches = patchify_images_and_masks(images, masks, patch_size, step)\n","\n","    filtered_images, filtered_masks = filter_non_empty_patches(image_patches, mask_patches)\n","    print(\"Filtered image shape:\", filtered_images.shape)\n","    print(\"Filtered mask shape:\", filtered_masks.shape)\n","\n","    resized_images, resized_masks = resize_patches(filtered_images, filtered_masks, target_size)\n","    print(\"Resized image shape:\", resized_images.shape)\n","    print(\"Resized mask shape:\", resized_masks.shape)\n","\n","    normalized_masks = normalize_masks(resized_masks)\n","    print(\"Normalized mask shape:\", normalized_masks.shape)\n","    print(f\"Normalized masks range: {normalized_masks.min()} - {normalized_masks.max()}\")\n","\n","    visualize_patches(resized_images, normalized_masks)\n","\n","    dataset = create_dataset(resized_images, normalized_masks)\n","    print(\"Dataset:\", dataset)\n","\n","    # Check shape and range of features in the dataset\n","    for i in range(10):\n","        img = np.array(dataset[i][\"image\"])  # Convert back to numpy array\n","        mask = np.array(dataset[i][\"label\"])  # Convert back to numpy array\n","        print(f\"Sample {i} - Image shape: {img.shape}, range: {img.min()} - {img.max()}\")\n","        print(f\"Sample {i} - Mask shape: {mask.shape}, range: {mask.min()} - {mask.max()}\")\n","\n","    return dataset\n","\n","if __name__ == \"__main__\":\n","    dataset = main()"]},{"cell_type":"markdown","metadata":{},"source":["# 3. Training Scripts"]},{"cell_type":"markdown","metadata":{},"source":["## 3.1 Train + Early Stopping"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from transformers import SamProcessor, SamModel\n","from tqdm import tqdm\n","from statistics import mean\n","import monai\n","from PIL import Image\n","import logging\n","import random\n","from torch.cuda.amp import GradScaler, autocast\n","\n","# Set random seed for reproducibility\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","class SAMDataset(Dataset):\n","    def __init__(self, dataset, processor):\n","        self.dataset = dataset\n","        self.processor = processor\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        item = self.dataset[idx]\n","        image = np.array(item[\"image\"])\n","        ground_truth_mask = np.array(item[\"label\"])\n","\n","        if len(image.shape) == 2:\n","            image = image.reshape(image.shape[0], image.shape[1], 1)\n","\n","        if image.shape[-1] == 1:\n","            image = np.repeat(image, 3, axis=-1)\n","\n","        prompt = self.get_bounding_box(ground_truth_mask)\n","        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\", do_rescale=False)\n","        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n","        inputs[\"ground_truth_mask\"] = torch.tensor(ground_truth_mask, dtype=torch.float32).unsqueeze(0)\n","\n","        return inputs\n","\n","    def get_bounding_box(self, ground_truth_map):\n","        y_indices, x_indices = np.where(np.squeeze(ground_truth_map) > 0)\n","        x_min, x_max = np.min(x_indices), np.max(x_indices)\n","        y_min, y_max = np.min(y_indices), np.max(y_indices)\n","        H, W = np.squeeze(ground_truth_map).shape\n","        x_min = max(0, x_min - np.random.randint(0, 20))\n","        x_max = min(W, x_max + np.random.randint(0, 20))\n","        y_min = max(0, y_min - np.random.randint(0, 20))\n","        y_max = min(H, y_max + np.random.randint(0, 20))\n","        return [x_min, y_min, x_max, y_max]\n","\n","def load_model(model_name):\n","    model = SamModel.from_pretrained(model_name)\n","    for name, param in model.named_parameters():\n","        if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n","            param.requires_grad_(False)\n","    return model\n","\n","def save_checkpoint(model, optimizer, epoch, losses, val_losses, checkpoint_path):\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'losses': losses,\n","        'val_losses': val_losses\n","    }\n","    torch.save(checkpoint, checkpoint_path)\n","\n","def load_checkpoint(model, optimizer, checkpoint_path):\n","    if os.path.isfile(checkpoint_path):\n","        checkpoint = torch.load(checkpoint_path)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        epoch = checkpoint['epoch']\n","        losses = checkpoint['losses']\n","        val_losses = checkpoint['val_losses']\n","        return epoch, losses, val_losses\n","    else:\n","        return 0, [], []\n","\n","def calculate_metrics(pred_masks, true_masks):\n","    pred_masks = (pred_masks > 0.5).float()\n","    intersection = (pred_masks * true_masks).sum()\n","    union = (pred_masks + true_masks).sum() - intersection\n","    iou = intersection / union\n","    precision = intersection / pred_masks.sum()\n","    recall = intersection / true_masks.sum()\n","    return iou.item(), precision.item(), recall.item()\n","\n","def validate(model, dataloader, seg_loss, device):\n","    model.eval()\n","    val_losses = []\n","    iou_scores = []\n","    precision_scores = []\n","    recall_scores = []\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n","                            input_boxes=batch[\"input_boxes\"].to(device),\n","                            multimask_output=False)\n","            predicted_masks = outputs.pred_masks.squeeze(1)\n","            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n","            loss = seg_loss(predicted_masks, ground_truth_masks)\n","            val_losses.append(loss.item())\n","\n","            iou, precision, recall = calculate_metrics(predicted_masks, ground_truth_masks)\n","            iou_scores.append(iou)\n","            precision_scores.append(precision)\n","            recall_scores.append(recall)\n","\n","    return mean(val_losses), mean(iou_scores), mean(precision_scores), mean(recall_scores)\n","\n","def adjust_params(current_epoch, losses, val_losses, checkpoint_interval, early_stop_patience):\n","    if len(losses) > 1 and len(val_losses) > 1:\n","        loss_delta = losses[-1] - losses[-2]\n","        val_loss_delta = val_losses[-1] - val_losses[-2]\n","\n","        # Adjust checkpoint interval based on training loss delta\n","        if abs(loss_delta) > 0.01:\n","            checkpoint_interval = max(1, checkpoint_interval - 1)\n","        else:\n","            checkpoint_interval += 1\n","\n","        # Adjust early stopping patience based on validation loss stability\n","        if abs(val_loss_delta) < 0.005:\n","            early_stop_patience = max(1, early_stop_patience - 1)\n","        else:\n","            early_stop_patience += 1\n","\n","    return checkpoint_interval, early_stop_patience\n","\n","def get_grad_norms(parameters):\n","    total_norm = 0.0\n","    for p in parameters:\n","        if p.grad is not None:\n","            param_norm = p.grad.data.norm(2)\n","            total_norm += param_norm.item() ** 2\n","    total_norm = total_norm ** 0.5\n","    return total_norm\n","\n","def adjust_grad_clip(grad_norms, grad_clip):\n","    if grad_norms > 5.0:\n","        grad_clip = max(0.1, grad_clip * 0.9)\n","    elif grad_norms < 0.5:\n","        grad_clip = min(5.0, grad_clip * 1.1)\n","    return grad_clip\n","\n","def train(model, train_loader, val_loader, optimizer, seg_loss, device, num_epochs, logger, start_epoch, losses, val_losses, checkpoint_interval, scheduler=None):\n","    best_val_loss = float('inf')\n","    epochs_no_improve = 0\n","    early_stop_patience = 10  # Initial value, will be adjusted dynamically\n","    grad_clip = 1.0  # Initial gradient clipping threshold\n","    scaler = GradScaler()  # Gradient scaler for automatic mixed precision\n","\n","    for epoch in range(start_epoch, num_epochs):\n","        model.train()\n","        epoch_losses = []\n","        grad_norms_list = []\n","        for batch_idx, batch in enumerate(tqdm(train_loader), 1):\n","            with autocast():\n","                outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n","                                input_boxes=batch[\"input_boxes\"].to(device),\n","                                multimask_output=False)\n","\n","                predicted_masks = outputs.pred_masks.squeeze(1)\n","                ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n","                loss = seg_loss(predicted_masks, ground_truth_masks)\n","\n","            optimizer.zero_grad()\n","            scaler.scale(loss).backward()\n","\n","            grad_norms = get_grad_norms(model.parameters())\n","            grad_norms_list.append(grad_norms)\n","            grad_clip = adjust_grad_clip(mean(grad_norms_list), grad_clip)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            epoch_losses.append(loss.item())\n","\n","            if batch_idx % 1600 == 0:\n","                logger.info(f'Epoch [{epoch + 1}/{num_epochs}], Batch [{batch_idx}/{len(train_loader)}], Loss: {loss.item()}, Grad Norms: {grad_norms}, Grad Clip: {grad_clip}')\n","\n","        mean_loss = mean(epoch_losses)\n","        losses.append(mean_loss)\n","        logger.info(f'EPOCH: {epoch + 1}/{num_epochs}, Mean Loss: {mean_loss}')\n","\n","        val_loss, val_iou, val_precision, val_recall = validate(model, val_loader, seg_loss, device)\n","        val_losses.append(val_loss)\n","        logger.info(f'EPOCH: {epoch + 1}/{num_epochs}, Val Loss: {val_loss}, IoU: {val_iou}, Precision: {val_precision}, Recall: {val_recall}')\n","\n","        checkpoint_interval, early_stop_patience = adjust_params(epoch + 1, losses, val_losses, checkpoint_interval, early_stop_patience)\n","\n","        if epoch % checkpoint_interval == 0:\n","            save_checkpoint(model, optimizer, epoch + 1, losses, val_losses, \"/home/ubuntu/models/videosam_checkpoint.pth\")\n","\n","        if scheduler:\n","            scheduler.step(val_loss)\n","\n","        # Early stopping with cooldown\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            epochs_no_improve = 0\n","            # Save the best model\n","            torch.save(model.state_dict(), \"/home/ubuntu/models/videosam_best.pth\")\n","        else:\n","            epochs_no_improve += 1\n","\n","        if epochs_no_improve >= early_stop_patience:\n","            logger.info(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation loss for {early_stop_patience} consecutive epochs\")\n","            break\n","\n","    return losses, val_losses\n","\n","def plot_losses(train_losses, val_losses):\n","    plt.figure(figsize=(10, 5))\n","    plt.plot(train_losses, marker='o', linestyle='-', color='b', label='Training Loss per Epoch')\n","    plt.plot(val_losses, marker='x', linestyle='--', color='r', label='Validation Loss per Epoch')\n","    plt.title('Loss Evolution Across Epochs', fontsize=14)\n","    plt.xlabel('Epochs', fontsize=12)\n","    plt.ylabel('Loss', fontsize=12)\n","    plt.grid(True)\n","    plt.legend(fontsize=12)\n","    plt.savefig('/home/ubuntu/plots/loss_evolution.jpg', dpi=300)\n","    plt.show()\n","\n","def main():\n","    global model, optimizer, current_epoch  # Declare global variables for signal handler access\n","\n","    # Set up logging\n","    logging.basicConfig(filename='/home/ubuntu/train.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","    logger = logging.getLogger()\n","\n","    # Ensure directories exist\n","    os.makedirs('/home/ubuntu/plots', exist_ok=True)\n","    os.makedirs('/home/ubuntu/models', exist_ok=True)\n","\n","    processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n","\n","    # Split dataset into training and validation sets\n","    dataset_length = len(dataset)\n","    train_length = int(0.8 * dataset_length)\n","    val_length = dataset_length - train_length\n","    train_dataset, val_dataset = random_split(dataset, [train_length, val_length])\n","    \n","    train_dataset = SAMDataset(dataset=train_dataset, processor=processor)\n","    val_dataset = SAMDataset(dataset=val_dataset, processor=processor)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=False)\n","    val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, drop_last=False)\n","\n","    model = load_model(\"facebook/sam-vit-base\")\n","    optimizer = torch.optim.Adam(model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n","    seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model.to(device)\n","\n","    # Optional: Learning rate scheduler\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1, verbose=True)\n","\n","    # Load from checkpoint if available\n","    checkpoint_path = \"/home/ubuntu/models/videosam_checkpoint.pth\"\n","    start_epoch, train_losses, val_losses = load_checkpoint(model, optimizer, checkpoint_path)\n","\n","    num_epochs = 100\n","    checkpoint_interval = 1  # Save checkpoint every 1 epoch\n","    train_losses, val_losses = train(model, train_loader, val_loader, optimizer, seg_loss, device, num_epochs, logger, start_epoch, train_losses, val_losses, checkpoint_interval, scheduler)\n","    plot_losses(train_losses, val_losses)\n","\n","    torch.save(model.state_dict(), \"/home/ubuntu/models/videosam_base.pth\")\n","\n","if __name__ == \"__main__\":\n","    set_seed(42)  # Set seed for reproducibility\n","    main()"]},{"cell_type":"markdown","metadata":{},"source":["## 3.2 Train + Hyperparameter Tuning"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from transformers import SamProcessor, SamModel\n","from tqdm import tqdm\n","from statistics import mean\n","import monai\n","from PIL import Image\n","import logging\n","import random\n","from torch.cuda.amp import GradScaler, autocast\n","import optuna\n","import time\n","\n","# Set random seed for reproducibility\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","class SAMDataset(Dataset):\n","    def __init__(self, dataset, processor):\n","        self.dataset = dataset\n","        self.processor = processor\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        item = self.dataset[idx]\n","        image = np.array(item[\"image\"])\n","        ground_truth_mask = np.array(item[\"label\"])\n","\n","        if len(image.shape) == 2:\n","            image = image.reshape(image.shape[0], image.shape[1], 1)\n","\n","        if image.shape[-1] == 1:\n","            image = np.repeat(image, 3, axis=-1)\n","\n","        prompt = self.get_bounding_box(ground_truth_mask)\n","        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\", do_rescale=False)\n","        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n","        inputs[\"ground_truth_mask\"] = torch.tensor(ground_truth_mask, dtype=torch.float32).unsqueeze(0)\n","\n","        return inputs\n","\n","    def get_bounding_box(self, ground_truth_map):\n","        y_indices, x_indices = np.where(np.squeeze(ground_truth_map) > 0)\n","        x_min, x_max = np.min(x_indices), np.max(x_indices)\n","        y_min, y_max = np.min(y_indices), np.max(y_indices)\n","        H, W = np.squeeze(ground_truth_map).shape\n","        x_min = max(0, x_min - np.random.randint(0, 20))\n","        x_max = min(W, x_max + np.random.randint(0, 20))\n","        y_min = max(0, y_min - np.random.randint(0, 20))\n","        y_max = min(H, y_max + np.random.randint(0, 20))\n","        return [x_min, y_min, x_max, y_max]\n","\n","def load_model(model_name):\n","    model = SamModel.from_pretrained(model_name)\n","    for name, param in model.named_parameters():\n","        if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n","            param.requires_grad_(False)\n","    return model\n","\n","def save_checkpoint(model, optimizer, epoch, losses, val_losses, checkpoint_path):\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'losses': losses,\n","        'val_losses': val_losses\n","    }\n","    torch.save(checkpoint, checkpoint_path)\n","\n","def load_checkpoint(model, optimizer, checkpoint_path):\n","    if os.path.isfile(checkpoint_path):\n","        checkpoint = torch.load(checkpoint_path)\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        epoch = checkpoint['epoch']\n","        losses = checkpoint['losses']\n","        val_losses = checkpoint['val_losses']\n","        return epoch, losses, val_losses\n","    else:\n","        return 0, [], []\n","\n","def calculate_metrics(pred_masks, true_masks):\n","    pred_masks = (pred_masks > 0.5).float()\n","    intersection = (pred_masks * true_masks).sum()\n","    union = (pred_masks + true_masks).sum() - intersection\n","    iou = intersection / union\n","    precision = intersection / pred_masks.sum()\n","    recall = intersection / true_masks.sum()\n","    return iou.item(), precision.item(), recall.item()\n","\n","def validate(model, dataloader, seg_loss, device):\n","    model.eval()\n","    val_losses = []\n","    iou_scores = []\n","    precision_scores = []\n","    recall_scores = []\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n","                            input_boxes=batch[\"input_boxes\"].to(device),\n","                            multimask_output=False)\n","            predicted_masks = outputs.pred_masks.squeeze(1)\n","            ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n","            loss = seg_loss(predicted_masks, ground_truth_masks)\n","            val_losses.append(loss.item())\n","\n","            iou, precision, recall = calculate_metrics(predicted_masks, ground_truth_masks)\n","            iou_scores.append(iou)\n","            precision_scores.append(precision)\n","            recall_scores.append(recall)\n","\n","    return mean(val_losses), mean(iou_scores), mean(precision_scores), mean(recall_scores)\n","\n","def adjust_params(current_epoch, losses, val_losses, checkpoint_interval, early_stop_patience):\n","    if len(losses) > 1 and len(val_losses) > 1:\n","        loss_delta = losses[-1] - losses[-2]\n","        val_loss_delta = val_losses[-1] - val_losses[-2]\n","\n","        # Adjust checkpoint interval based on training loss delta\n","        if abs(loss_delta) > 0.01:\n","            checkpoint_interval = max(1, checkpoint_interval - 1)\n","        else:\n","            checkpoint_interval += 1\n","\n","        # Adjust early stopping patience based on validation loss stability\n","        if abs(val_loss_delta) < 0.005:\n","            early_stop_patience = max(1, early_stop_patience - 1)\n","        else:\n","            early_stop_patience += 1\n","\n","    return checkpoint_interval, early_stop_patience\n","\n","def get_grad_norms(parameters):\n","    total_norm = 0.0\n","    for p in parameters:\n","        if p.grad is not None:\n","            param_norm = p.grad.data.norm(2)\n","            total_norm += param_norm.item() ** 2\n","    total_norm = total_norm ** 0.5\n","    return total_norm\n","\n","def adjust_grad_clip(grad_norms, grad_clip):\n","    if grad_norms > 5.0:\n","        grad_clip = max(0.1, grad_clip * 0.9)\n","    elif grad_norms < 0.5:\n","        grad_clip = min(5.0, grad_clip * 1.1)\n","    return grad_clip\n","\n","def objective(trial):\n","    start_time = time.time()  # Track the start time of the trial\n","\n","    # Define hyperparameters to tune\n","    lr = trial.suggest_float('lr', 1e-6, 1e-4, log=True)\n","    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-3, log=True)\n","    batch_size = trial.suggest_int('batch_size', 4, 16)\n","    patience = trial.suggest_int('patience', 5, 20)\n","    processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n","    \n","    model = load_model(\"facebook/sam-vit-base\")\n","    optimizer = torch.optim.Adam(model.mask_decoder.parameters(), lr=lr, weight_decay=weight_decay)\n","    seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n","    \n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model.to(device)\n","    \n","    # Split dataset into training and validation sets\n","    dataset_length = len(dataset)\n","    train_length = int(0.8 * dataset_length)\n","    val_length = dataset_length - train_length\n","    train_dataset, val_dataset = random_split(dataset, [train_length, val_length])\n","    \n","    train_dataset = SAMDataset(dataset=train_dataset, processor=processor)\n","    val_dataset = SAMDataset(dataset=val_dataset, processor=processor)\n","    \n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n","    \n","    # Training loop with early stopping\n","    num_epochs = 100\n","    best_val_loss = float('inf')\n","    epochs_no_improve = 0\n","    grad_clip = 1.0  # Initialize grad_clip\n","    scaler = GradScaler()\n","    train_losses = []\n","    val_losses = []\n","    val_iou_scores = []\n","    val_precision_scores = []\n","    val_recall_scores = []\n","\n","    # Log the current trial parameters\n","    logging.info(f\"Starting trial with lr={lr}, weight_decay={weight_decay}, batch_size={batch_size}, patience={patience}\")\n","    \n","    for epoch in range(num_epochs):\n","        model.train()\n","        epoch_losses = []\n","        logging.info(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n","        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n","            with autocast():\n","                outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n","                                input_boxes=batch[\"input_boxes\"].to(device),\n","                                multimask_output=False)\n","\n","                predicted_masks = outputs.pred_masks.squeeze(1)\n","                ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n","                loss = seg_loss(predicted_masks, ground_truth_masks)\n","\n","            optimizer.zero_grad()\n","            scaler.scale(loss).backward()\n","\n","            grad_norms = get_grad_norms(model.parameters())\n","            grad_clip = adjust_grad_clip(grad_norms, grad_clip)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n","\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            epoch_losses.append(loss.item())\n","\n","        mean_loss = mean(epoch_losses)\n","        train_losses.append(mean_loss)\n","        logging.info(f\"Epoch {epoch + 1}/{num_epochs} completed. Training loss: {mean_loss:.4f}\")\n","        \n","        val_loss, val_iou, val_precision, val_recall = validate(model, val_loader, seg_loss, device)\n","        val_losses.append(val_loss)\n","        val_iou_scores.append(val_iou)\n","        val_precision_scores.append(val_precision)\n","        val_recall_scores.append(val_recall)\n","        \n","        logging.info(f\"Validation results - Loss: {val_loss:.4f}, IoU: {val_iou:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n","        \n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            epochs_no_improve = 0\n","        else:\n","            epochs_no_improve += 1\n","\n","        if epochs_no_improve >= patience:\n","            logging.info(f\"Early stopping at epoch {epoch + 1}/{num_epochs} due to no improvement for {patience} epochs\")\n","            break\n","\n","    end_time = time.time()  # Track the end time of the trial\n","    elapsed_time = end_time - start_time\n","    logging.info(f\"Trial completed in {elapsed_time:.2f} seconds with best validation loss: {best_val_loss:.4f}\")\n","\n","    return best_val_loss\n","\n","def plot_losses_and_metrics(train_losses, val_losses, val_iou, val_precision, val_recall):\n","    epochs = range(1, len(train_losses) + 1)\n","    \n","    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n","    \n","    # Plot Training and Validation Losses\n","    axes[0, 0].plot(epochs, train_losses, 'b-', label='Training Loss')\n","    axes[0, 0].plot(epochs, val_losses, 'r--', label='Validation Loss')\n","    axes[0, 0].set_title('Loss Evolution Across Epochs')\n","    axes[0, 0].set_xlabel('Epochs')\n","    axes[0, 0].set_ylabel('Loss')\n","    axes[0, 0].legend()\n","    axes[0, 0].grid(True)\n","    \n","    # Plot IoU Scores\n","    axes[0, 1].plot(epochs, val_iou, 'g-', label='Validation IoU')\n","    axes[0, 1].set_title('IoU Scores Across Epochs')\n","    axes[0, 1].set_xlabel('Epochs')\n","    axes[0, 1].set_ylabel('IoU')\n","    axes[0, 1].legend()\n","    axes[0, 1].grid(True)\n","    \n","    # Plot Precision Scores\n","    axes[1, 0].plot(epochs, val_precision, 'm-', label='Validation Precision')\n","    axes[1, 0].set_title('Precision Scores Across Epochs')\n","    axes[1, 0].set_xlabel('Epochs')\n","    axes[1, 0].set_ylabel('Precision')\n","    axes[1, 0].legend()\n","    axes[1, 0].grid(True)\n","    \n","    # Plot Recall Scores\n","    axes[1, 1].plot(epochs, val_recall, 'c-', label='Validation Recall')\n","    axes[1, 1].set_title('Recall Scores Across Epochs')\n","    axes[1, 1].set_xlabel('Epochs')\n","    axes[1, 1].set_ylabel('Recall')\n","    axes[1, 1].legend()\n","    axes[1, 1].grid(True)\n","    \n","    plt.tight_layout()\n","    plt.savefig('/home/ubuntu/plots/losses_and_metrics_evolution.jpg', dpi=300)\n","    plt.show()\n","\n","def main():\n","    global model, optimizer, current_epoch\n","\n","    # Set up logging\n","    logging.basicConfig(filename='/home/ubuntu/train.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","    logger = logging.getLogger()\n","\n","    # Ensure directories exist\n","    os.makedirs('/home/ubuntu/plots', exist_ok=True)\n","    os.makedirs('/home/ubuntu/models', exist_ok=True)\n","\n","    processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n","\n","    # Run hyperparameter optimization\n","    n_trials = 50\n","    study = optuna.create_study(direction='minimize')\n","    for trial in range(n_trials):\n","        logger.info(f\"Starting trial {trial + 1}/{n_trials}\")\n","        start_time = time.time()  # Track the start time of the trial\n","        study.optimize(objective, n_trials=1, catch=(Exception,))\n","        end_time = time.time()  # Track the end time of the trial\n","        elapsed_time = end_time - start_time\n","        logger.info(f\"Completed trial {trial + 1}/{n_trials} in {elapsed_time:.2f} seconds\")\n","        logger.info(f\"Best validation loss so far: {study.best_value:.4f}\")\n","        remaining_trials = n_trials - (trial + 1)\n","        logger.info(f\"Remaining trials: {remaining_trials}\")\n","\n","    # Print the best hyperparameters\n","    print(\"Best hyperparameters:\", study.best_params)\n","\n","    # Train the final model with the best hyperparameters\n","    best_params = study.best_params\n","    model = load_model(\"facebook/sam-vit-base\")\n","    optimizer = torch.optim.Adam(model.mask_decoder.parameters(), lr=best_params['lr'], weight_decay=best_params['weight_decay'])\n","    seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n","\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    model.to(device)\n","\n","    # Split dataset into training and validation sets\n","    dataset_length = len(dataset)\n","    train_length = int(0.8 * dataset_length)\n","    val_length = dataset_length - train_length\n","    train_dataset, val_dataset = random_split(dataset, [train_length, val_length])\n","    \n","    train_dataset = SAMDataset(dataset=train_dataset, processor=processor)\n","    val_dataset = SAMDataset(dataset=val_dataset, processor=processor)\n","    \n","    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, drop_last=False)\n","    val_loader = DataLoader(val_dataset, batch_size=best_params['batch_size'], shuffle=False, drop_last=False)\n","\n","    # Optional: Learning rate scheduler\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.1, verbose=True)\n","\n","    # Load from checkpoint if available\n","    checkpoint_path = \"/home/ubuntu/models/videosam_checkpoint.pth\"\n","    start_epoch, train_losses, val_losses = load_checkpoint(model, optimizer, checkpoint_path)\n","\n","    num_epochs = 100\n","    checkpoint_interval = 1  # Save checkpoint every 1 epoch\n","    train_losses, val_losses, val_iou_scores, val_precision_scores, val_recall_scores = train(model, train_loader, val_loader, optimizer, seg_loss, device, num_epochs, logger, start_epoch, train_losses, val_losses, checkpoint_interval, scheduler)\n","    plot_losses_and_metrics(train_losses, val_losses, val_iou_scores, val_precision_scores, val_recall_scores)\n","\n","    torch.save(model.state_dict(), \"/home/ubuntu/models/videosam_base.pth\")\n","\n","if __name__ == \"__main__\":\n","    set_seed(42)  # Set seed for reproducibility\n","    main()\n"]},{"cell_type":"markdown","metadata":{},"source":["# 4 Inference"]},{"cell_type":"markdown","metadata":{"id":"yva6w7yrBgob"},"source":["## 4.1 Mask Extraction (Single Frame)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":672},"executionInfo":{"elapsed":9444,"status":"ok","timestamp":1714904592317,"user":{"displayName":"Chika Maduabuchi","userId":"13808453024378790419"},"user_tz":240},"id":"3JBowQrAWvK3","outputId":"ae43155f-0b7f-4cc8-e188-c4aaf7eadf47"},"outputs":[],"source":["import numpy as np\n","import torch\n","import tifffile\n","import matplotlib.pyplot as plt\n","from transformers import SamModel, SamProcessor\n","from torch.utils.data import Dataset, DataLoader\n","import os\n","\n","class SAMInferenceDataset(Dataset):\n","    def __init__(self, images, masks, processor, grid_size=256):\n","        self.images = images\n","        self.masks = masks\n","        self.processor = processor\n","        self.grid_size = grid_size\n","\n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = self.images[idx]\n","        mask = self.masks[idx]\n","        image_rgb = np.repeat(image[:, :, np.newaxis], 3, axis=2)  # Convert grayscale to RGB\n","        grid_boxes = self.get_grid_boxes(image_rgb.shape[0], image_rgb.shape[1], self.grid_size)\n","        inputs = self.processor(image_rgb, input_boxes=grid_boxes, return_tensors=\"pt\", do_rescale=False)\n","        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n","        return inputs, mask\n","\n","    @staticmethod\n","    def get_grid_boxes(image_height, image_width, grid_size):\n","        grid_boxes = []\n","        for y in range(0, image_height, grid_size):\n","            for x in range(0, image_width, grid_size):\n","                x_min = x\n","                x_max = min(x + grid_size, image_width)\n","                y_min = y\n","                y_max = min(y + grid_size, image_height)\n","                grid_boxes.append([[float(x_min), float(y_min), float(x_max), float(y_max)]])\n","        return grid_boxes\n","\n","def load_and_prepare_data(image_path, mask_path):\n","    images = tifffile.imread(image_path)\n","    masks = tifffile.imread(mask_path)\n","    return images, masks\n","\n","def predict(model, inputs):\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model(**inputs, multimask_output=False)\n","        probabilities = torch.sigmoid(outputs.pred_masks.squeeze(1)).cpu().numpy().squeeze()\n","    return (probabilities > 0.5).astype(np.uint8)\n","\n","def plot_results(fig, axes, images, titles, row_index):\n","    for col_index, (image, title) in enumerate(zip(images, titles)):\n","        ax = axes[row_index, col_index]\n","        ax.imshow(image, cmap='gray')\n","        ax.set_title(title)\n","        ax.set_xticks([])\n","        ax.set_yticks([])\n","\n","def stitch_masks(masks, image_shape, grid_size):\n","    stitched_mask = np.zeros(image_shape, dtype=np.uint8)\n","    mask_idx = 0\n","    for y in range(0, image_shape[0], grid_size):\n","        for x in range(0, image_shape[1], grid_size):\n","            stitched_mask[y:y+grid_size, x:x+grid_size] = masks[mask_idx]\n","            mask_idx += 1\n","    return stitched_mask\n","\n","def main():\n","    # Paths to the test images and masks\n","    image_path = \"/home/ubuntu/test/test.tif\"\n","    mask_path = \"/home/ubuntu/test/test_mask.tif\"\n","    fluid_names = [\"Water\", \"FC72\", \"Nitrogen\", \"Argon\"]\n","    output_dir = \"/home/ubuntu/masks\"\n","\n","    # Ensure output directory exists\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Load and prepare the data\n","    large_test_images, ground_truth_masks = load_and_prepare_data(image_path, mask_path)\n","    \n","    # Create the figure for plotting\n","    fig, axes = plt.subplots(4, 4, figsize=(20, 20))\n","\n","    # Load models and processor\n","    videosam_model_path = \"/home/ubuntu/models/videosam_base.pth\"\n","    sam_model_name = \"facebook/sam-vit-base\"\n","    videosam_model = SamModel.from_pretrained(sam_model_name)\n","    videosam_model.load_state_dict(torch.load(videosam_model_path))\n","    sam_model = SamModel.from_pretrained(sam_model_name)\n","    processor = SamProcessor.from_pretrained(sam_model_name)\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    videosam_model.to(device)\n","    sam_model.to(device)\n","\n","    # Ensure randomness for each run\n","    np.random.seed()\n","\n","    grid_size = 256\n","\n","    # Create datasets\n","    start_end_indices = [(i * 250, (i + 1) * 250) for i in range(4)]\n","    for i, (start_index, end_index) in enumerate(start_end_indices):\n","        random_frame_index = np.random.randint(start_index, end_index)\n","        test_image = large_test_images[random_frame_index]\n","        ground_truth_mask = ground_truth_masks[random_frame_index]\n","        test_image_rgb = np.repeat(test_image[:, :, np.newaxis], 3, axis=2)\n","\n","        grid_boxes = SAMInferenceDataset.get_grid_boxes(test_image_rgb.shape[0], test_image_rgb.shape[1], grid_size)\n","        inputs = processor(test_image_rgb, input_boxes=grid_boxes, return_tensors=\"pt\", do_rescale=False)\n","        inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","        # Debugging: Print input details\n","        print(f\"Fluid: {fluid_names[i]}, Grid Boxes: {grid_boxes}\")\n","        print(f\"Input shapes: {[v.shape for v in inputs.values()]}\")\n","\n","        # Predictions\n","        videosam_predictions = []\n","        sam_predictions = []\n","        for box in grid_boxes:\n","            single_input = {k: v for k, v in inputs.items()}\n","            videosam_prediction = predict(videosam_model, single_input)\n","            sam_prediction = predict(sam_model, single_input)\n","            videosam_predictions.append(videosam_prediction)\n","            sam_predictions.append(sam_prediction)\n","\n","        # Stitch masks\n","        videosam_stitched_mask = stitch_masks(videosam_predictions, test_image.shape, grid_size)\n","        sam_stitched_mask = stitch_masks(sam_predictions, test_image.shape, grid_size)\n","\n","        # Save masks\n","        np.save(os.path.join(output_dir, f\"{fluid_names[i]}_videosam_mask.npy\"), videosam_stitched_mask)\n","        np.save(os.path.join(output_dir, f\"{fluid_names[i]}_sam_mask.npy\"), sam_stitched_mask)\n","        np.save(os.path.join(output_dir, f\"{fluid_names[i]}_ground_truth_mask.npy\"), ground_truth_mask)\n","\n","        # Debug print to check for blank masks\n","        print(f\"Fluid: {fluid_names[i]}, VideoSAM unique values: {np.unique(videosam_stitched_mask)}, SAM unique values: {np.unique(sam_stitched_mask)}\")\n","\n","        images = [test_image, videosam_stitched_mask, sam_stitched_mask, ground_truth_mask]\n","        titles = [f\"{fluid_names[i]} Original Image\", \"VideoSAM Binary Mask\", \"SAM Binary Mask\", \"Ground Truth Mask\"]\n","        plot_results(fig, axes, images, titles, i)\n","\n","    plt.tight_layout()\n","    plt.savefig('/home/ubuntu/plots/visual_comparison_test.jpg', dpi=300)\n","    plt.show()\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","metadata":{"id":"9jXCrRk-BQsA"},"source":["## 4.2 Metrics Evaluation (Single Frame)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":309},"executionInfo":{"elapsed":6833,"status":"ok","timestamp":1714904622575,"user":{"displayName":"Chika Maduabuchi","userId":"13808453024378790419"},"user_tz":240},"id":"FSbM-mbHu7kx","outputId":"6904ae20-f908-4ecd-fbb4-5b1ba59dc11c"},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, jaccard_score, confusion_matrix\n","\n","def calculate_specificity(y_true, y_pred):\n","    tn, fp, _, _ = confusion_matrix(y_true, y_pred).ravel()\n","    return tn / (tn + fp) if (tn + fp) > 0 else 0\n","\n","def calculate_dice_coefficient(y_true, y_pred):\n","    precision = precision_score(y_true, y_pred, zero_division=0)\n","    recall = recall_score(y_true, y_pred, zero_division=0)\n","    return 2 * (precision * recall) / (precision + recall + 1e-7) if (precision + recall) > 0 else 0\n","\n","def load_masks(fluid_name, base_dir):\n","    videosam_mask = np.load(os.path.join(base_dir, f\"{fluid_name}_videosam_mask.npy\"))\n","    sam_mask = np.load(os.path.join(base_dir, f\"{fluid_name}_sam_mask.npy\"))\n","    gt_mask = np.load(os.path.join(base_dir, f\"{fluid_name}_ground_truth_mask.npy\"))\n","    return videosam_mask, sam_mask, gt_mask\n","\n","def binarize_masks(mask):\n","    return (mask > 0.5).astype(np.uint8)\n","\n","def calculate_metrics(y_true, y_pred):\n","    return {\n","        'Accuracy': accuracy_score(y_true, y_pred),\n","        'Precision': precision_score(y_true, y_pred, zero_division=0),\n","        'Specificity': calculate_specificity(y_true, y_pred),\n","        'F1 Score': f1_score(y_true, y_pred, zero_division=0),\n","        'IoU': jaccard_score(y_true, y_pred, zero_division=0),\n","        'Dice': calculate_dice_coefficient(y_true, y_pred)\n","    }\n","\n","def plot_all_metrics(results, output_path_base):\n","    sns.set(style=\"whitegrid\")\n","    sns.set_context(\"talk\", font_scale=1.2)\n","    custom_palette = [\"#3498db\", \"#e74c3c\"]\n","    sns.set_palette(custom_palette)\n","\n","    metrics = ['Accuracy', 'Precision', 'Specificity', 'F1 Score', 'IoU', 'Dice']\n","    data = {'Fluid': [], 'Metric': [], 'Value': [], 'Model': []}\n","\n","    for fluid in results:\n","        for metric in metrics:\n","            data['Fluid'].extend([fluid] * 2)\n","            data['Metric'].extend([metric] * 2)\n","            data['Value'].extend([results[fluid]['VideoSAM'][metric], results[fluid]['SAM'][metric]])\n","            data['Model'].extend(['VideoSAM', 'SAM'])\n","            \n","    df = pd.DataFrame(data)\n","    g = sns.catplot(x='Metric', y='Value', hue='Model', col='Fluid', data=df, kind='bar', height=5, aspect=1, legend_out=True)\n","    g.set_titles(\"{col_name} Dataset\")\n","    g.set_axis_labels(\"\", \"Score\")\n","\n","    for ax in g.axes.flat:\n","        ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n","\n","    g.fig.subplots_adjust(top=0.9)\n","    g.fig.legends[0].set_title('Model')\n","    g.fig.legends[0].set_bbox_to_anchor((0.94, 0.7))\n","\n","    g.fig.tight_layout()\n","    \n","    output_path_png = f\"{output_path_base}.png\"\n","    output_path_pdf = f\"{output_path_base}.pdf\"\n","    \n","    plt.savefig(output_path_png, dpi=300)\n","    plt.savefig(output_path_pdf)\n","    plt.show()\n","\n","def create_table(results, output_path):\n","    metrics = ['Accuracy', 'Precision', 'Specificity', 'F1 Score', 'IoU', 'Dice']\n","    table_data = []\n","\n","    for fluid in results:\n","        for model in ['VideoSAM', 'SAM']:\n","            row = [fluid, model]\n","            for metric in metrics:\n","                row.append(results[fluid][model][metric])\n","            table_data.append(row)\n","\n","    df = pd.DataFrame(table_data, columns=['Fluid', 'Model'] + metrics)\n","    df.to_csv(output_path, index=False)\n","    print(df)\n","    return df\n","\n","def main():\n","    base_dir = \"/home/ubuntu/masks\"\n","    fluid_names = [\"Nitrogen\", \"FC72\", \"Water\", \"Argon\"]\n","    results = {fluid: {'VideoSAM': {}, 'SAM': {}} for fluid in fluid_names}\n","\n","    for fluid in fluid_names:\n","        videosam_mask, sam_mask, gt_mask = load_masks(fluid, base_dir)\n","        \n","        # Binarize masks\n","        videosam_mask = binarize_masks(videosam_mask)\n","        sam_mask = binarize_masks(sam_mask)\n","        gt_mask = binarize_masks(gt_mask)\n","        \n","        results[fluid]['VideoSAM'] = calculate_metrics(gt_mask.flatten(), videosam_mask.flatten())\n","        results[fluid]['SAM'] = calculate_metrics(gt_mask.flatten(), sam_mask.flatten())\n","\n","    output_path_base = '/home/ubuntu/plots/metrics_comparison_all'\n","    plot_all_metrics(results, output_path_base)\n","\n","    table_output_path = '/home/ubuntu/plots/metrics_comparison_table.csv'\n","    metrics_table = create_table(results, table_output_path)\n","\n","    return metrics_table\n","\n","if __name__ == \"__main__\":\n","    metrics_table = main()\n","    display(metrics_table)\n"]},{"cell_type":"markdown","metadata":{},"source":["## 4.1 Mask Extraction (Composite Frames)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import torch\n","import tifffile\n","from transformers import SamModel, SamProcessor\n","import h5py\n","import os\n","import logging\n","from tqdm.notebook import tqdm\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","\n","class SAMInferenceDataset:\n","    @staticmethod\n","    def get_grid_boxes(image_height, image_width, grid_size):\n","        grid_boxes = []\n","        for y in range(0, image_height, grid_size):\n","            for x in range(0, image_width, grid_size):\n","                x_min = x\n","                x_max = min(x + grid_size, image_width)\n","                y_min = y\n","                y_max = min(y + grid_size, image_height)\n","                grid_boxes.append([[float(x_min), float(y_min), float(x_max), float(y_max)]])\n","        return grid_boxes\n","\n","def load_and_prepare_data(image_path, mask_path):\n","    try:\n","        images = tifffile.imread(image_path)\n","        masks = tifffile.imread(mask_path)\n","        return images, masks\n","    except Exception as e:\n","        logging.error(f\"Error loading data: {e}\")\n","        raise\n","\n","def predict(model, inputs):\n","    model.eval()\n","    with torch.no_grad():\n","        try:\n","            outputs = model(**inputs, multimask_output=False)\n","            probabilities = torch.sigmoid(outputs.pred_masks.squeeze(1)).cpu().numpy().squeeze()\n","            return (probabilities > 0.5).astype(np.uint8)\n","        except Exception as e:\n","            logging.error(f\"Error during prediction: {e}\")\n","            raise\n","\n","def stitch_masks(masks, image_shape, grid_size):\n","    stitched_mask = np.zeros(image_shape, dtype=np.uint8)\n","    mask_idx = 0\n","    for y in range(0, image_shape[0], grid_size):\n","        for x in range(0, image_shape[1], grid_size):\n","            stitched_mask[y:y+grid_size, x:x+grid_size] = masks[mask_idx]\n","            mask_idx += 1\n","    return stitched_mask\n","\n","def main():\n","    # Paths to the test images and masks\n","    image_path = \"/home/ubuntu/test/test.tif\"\n","    mask_path = \"/home/ubuntu/test/test_mask.tif\"\n","    fluid_names = [\"Water\", \"FC72\", \"Nitrogen\", \"Argon\"]\n","    output_dir = \"/home/ubuntu/masks\"\n","    \n","    # Ensure output directory exists\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Load and prepare the data\n","    large_test_images, ground_truth_masks = load_and_prepare_data(image_path, mask_path)\n","    logging.info(\"Data loaded successfully\")\n","\n","    # Load models and processor\n","    videosam_model_path = \"/home/ubuntu/models/videosam_base.pth\"\n","    sam_model_name = \"facebook/sam-vit-base\"\n","    videosam_model = SamModel.from_pretrained(sam_model_name)\n","    videosam_model.load_state_dict(torch.load(videosam_model_path))\n","    sam_model = SamModel.from_pretrained(sam_model_name)\n","    processor = SamProcessor.from_pretrained(sam_model_name)\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    videosam_model.to(device)\n","    sam_model.to(device)\n","    logging.info(\"Models loaded and moved to device\")\n","\n","    grid_size = 256\n","\n","    # Open an HDF5 file for storing the masks\n","    with h5py.File(os.path.join(output_dir, 'segmentation_masks.h5'), 'w') as h5f:\n","        for frame_index in tqdm(range(1000), desc=\"Processing frames\"):\n","            try:\n","                fluid_index = frame_index // 250\n","                fluid_name = fluid_names[fluid_index]\n","                \n","                logging.info(f\"Processing frame {frame_index} for fluid {fluid_name}\")\n","\n","                test_image = large_test_images[frame_index]\n","                ground_truth_mask = ground_truth_masks[frame_index]\n","                test_image_rgb = np.repeat(test_image[:, :, np.newaxis], 3, axis=2)\n","\n","                grid_boxes = SAMInferenceDataset.get_grid_boxes(test_image_rgb.shape[0], test_image_rgb.shape[1], grid_size)\n","                inputs = processor(test_image_rgb, input_boxes=grid_boxes, return_tensors=\"pt\", do_rescale=False)\n","                inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","                # Predictions\n","                videosam_predictions = []\n","                sam_predictions = []\n","                for box in grid_boxes:\n","                    single_input = {k: v for k, v in inputs.items()}\n","                    videosam_prediction = predict(videosam_model, single_input)\n","                    sam_prediction = predict(sam_model, single_input)\n","                    videosam_predictions.append(videosam_prediction)\n","                    sam_predictions.append(sam_prediction)\n","\n","                # Stitch masks\n","                videosam_stitched_mask = stitch_masks(videosam_predictions, test_image.shape, grid_size)\n","                sam_stitched_mask = stitch_masks(sam_predictions, test_image.shape, grid_size)\n","\n","                # Save masks to HDF5 file\n","                h5f.create_dataset(f\"{fluid_name}/videosam_mask_{frame_index % 250}\", data=videosam_stitched_mask, compression=\"gzip\")\n","                h5f.create_dataset(f\"{fluid_name}/sam_mask_{frame_index % 250}\", data=sam_stitched_mask, compression=\"gzip\")\n","                h5f.create_dataset(f\"{fluid_name}/ground_truth_mask_{frame_index % 250}\", data=ground_truth_mask, compression=\"gzip\")\n","                \n","                logging.info(f\"Processed and saved masks for frame {frame_index}\")\n","            except Exception as e:\n","                logging.error(f\"Error processing frame {frame_index}: {e}\")\n","\n","if __name__ == \"__main__\":\n","    main()\n"]},{"cell_type":"markdown","metadata":{},"source":["## 4.3 Metrics Evaluation (Composite Frames)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import h5py\n","from sklearn.metrics import f1_score, jaccard_score\n","from tqdm.notebook import tqdm\n","\n","def load_masks(fluid_name, frame_index, h5f):\n","    try:\n","        videosam_mask = h5f[f\"{fluid_name}/videosam_mask_{frame_index}\"][:]\n","    except KeyError:\n","        videosam_mask = None\n","\n","    try:\n","        sam_mask = h5f[f\"{fluid_name}/sam_mask_{frame_index}\"][:]\n","    except KeyError:\n","        sam_mask = None\n","\n","    try:\n","        gt_mask = h5f[f\"{fluid_name}/ground_truth_mask_{frame_index}\"][:]\n","    except KeyError:\n","        gt_mask = None\n","\n","    return videosam_mask, sam_mask, gt_mask\n","\n","def binarize_masks(mask):\n","    return (mask > 0.5).astype(np.uint8) if mask is not None else None\n","\n","def calculate_metrics(y_true, y_pred):\n","    if y_true is None or y_pred is None:\n","        return {'IoU': None, 'F1 Score': None}\n","    return {\n","        'IoU': jaccard_score(y_true, y_pred, zero_division=0),\n","        'F1 Score': f1_score(y_true, y_pred, zero_division=0)\n","    }\n","\n","def summarize_metrics(metrics_list):\n","    summary = {}\n","    for metric in metrics_list[0]:\n","        values = [metrics[metric] for metrics in metrics_list if metrics[metric] is not None]\n","        summary[metric] = {\n","            'Mean': np.mean(values) if values else 0,\n","            'Min': np.min(values) if values else 0,\n","            'Max': np.max(values) if values else 0,\n","            'StdDev': np.std(values) if values else 0\n","        }\n","    return summary\n","\n","def plot_metrics_summary(metrics_summary, output_path_base):\n","    sns.set(style=\"whitegrid\")\n","    sns.set_context(\"talk\", font_scale=1.2)\n","    custom_palette = [\"#3498db\", \"#e74c3c\"]\n","    sns.set_palette(custom_palette)\n","\n","    metrics = list(metrics_summary[next(iter(metrics_summary))]['VideoSAM'].keys())\n","    data = {'Fluid': [], 'Metric': [], 'Value': [], 'Model': [], 'Statistic': []}\n","\n","    for fluid in metrics_summary:\n","        for model in ['VideoSAM', 'SAM']:\n","            for metric in metrics:\n","                for stat in ['Mean', 'Min', 'Max', 'StdDev']:\n","                    value = metrics_summary[fluid][model][metric][stat]\n","                    if value is not None:\n","                        data['Fluid'].append(fluid)\n","                        data['Metric'].append(metric)\n","                        data['Value'].append(value)\n","                        data['Model'].append(model)\n","                        data['Statistic'].append(stat)\n","\n","    df = pd.DataFrame(data)\n","    g = sns.catplot(x='Statistic', y='Value', hue='Model', col='Fluid', row='Metric', data=df, kind='bar', height=4, aspect=1.5, legend_out=False)\n","    g.set_titles(\"{col_name} - {row_name}\")\n","    g.set_axis_labels(\"\", \"Score\")\n","\n","    for ax in g.axes.flat:\n","        ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n","\n","    g.fig.subplots_adjust(top=0.9)\n","    g.add_legend(title='Model')\n","\n","    # Ensure the output directory exists\n","    os.makedirs(os.path.dirname(output_path_base), exist_ok=True)\n","    \n","    output_path_png = f\"{output_path_base}.png\"\n","    output_path_pdf = f\"{output_path_base}.pdf\"\n","    \n","    plt.savefig(output_path_png, dpi=300)\n","    plt.savefig(output_path_pdf)\n","    plt.show()\n","\n","def create_summary_table(metrics_summary, output_path):\n","    metrics = list(metrics_summary[next(iter(metrics_summary))]['VideoSAM'].keys())\n","    table_data = []\n","\n","    for fluid in metrics_summary:\n","        row = [fluid]\n","        for model in ['VideoSAM', 'SAM']:\n","            for metric in metrics:\n","                stats = metrics_summary[fluid][model][metric]\n","                row.extend([stats['Mean'], stats['Min'], stats['Max'], stats['StdDev']])\n","        table_data.append(row)\n","\n","    columns = ['Fluid']\n","    for model in ['VideoSAM', 'SAM']:\n","        for metric in metrics:\n","            columns.extend([f'{model}_{metric}_Mean', f'{model}_{metric}_Min', f'{model}_{metric}_Max', f'{model}_{metric}_StdDev'])\n","\n","    df = pd.DataFrame(table_data, columns=columns)\n","    \n","    # Ensure the output directory exists\n","    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","    \n","    df.to_csv(output_path, index=False)\n","    print(df)\n","    return df\n","\n","def main():\n","    base_dir = \"/home/ubuntu/masks\"\n","    fluid_names = [\"Water\", \"FC72\", \"Nitrogen\", \"Argon\"]\n","    results = {fluid: {'VideoSAM': [], 'SAM': []} for fluid in fluid_names}\n","\n","    with h5py.File(os.path.join(base_dir, 'segmentation_masks.h5'), 'r') as h5f:\n","        for fluid in fluid_names:\n","            for frame_index in tqdm(range(250), desc=f\"Processing {fluid}\"):\n","                videosam_mask, sam_mask, gt_mask = load_masks(fluid, frame_index, h5f)\n","                \n","                # Binarize masks\n","                videosam_mask = binarize_masks(videosam_mask)\n","                sam_mask = binarize_masks(sam_mask)\n","                gt_mask = binarize_masks(gt_mask)\n","                \n","                results[fluid]['VideoSAM'].append(calculate_metrics(gt_mask.flatten() if gt_mask is not None else None,\n","                                                                    videosam_mask.flatten() if videosam_mask is not None else None))\n","                results[fluid]['SAM'].append(calculate_metrics(gt_mask.flatten() if gt_mask is not None else None,\n","                                                               sam_mask.flatten() if sam_mask is not None else None))\n","\n","    metrics_summary = {fluid: {'VideoSAM': summarize_metrics(results[fluid]['VideoSAM']),\n","                               'SAM': summarize_metrics(results[fluid]['SAM'])} for fluid in fluid_names}\n","\n","    # Print metrics_summary to debug\n","    print(\"Metrics Summary:\")\n","    for fluid, summary in metrics_summary.items():\n","        print(f\"Fluid: {fluid}\")\n","        for model, metrics in summary.items():\n","            print(f\"  Model: {model}\")\n","            for metric, values in metrics.items():\n","                print(f\"    Metric: {metric} - {values}\")\n","\n","    output_path_base = '/home/ubuntu/plots/metrics_comparison_summary'\n","    plot_metrics_summary(metrics_summary, output_path_base)\n","\n","    table_output_path = '/home/ubuntu/plots/metrics_comparison_summary_table.csv'\n","    summary_table = create_summary_table(metrics_summary, table_output_path)\n","\n","    return summary_table\n","\n","if __name__ == \"__main__\":\n","    summary_table = main()\n","    display(summary_table)\n"]},{"cell_type":"markdown","metadata":{},"source":["# 5. Training Script + Different Strategies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":918},"executionInfo":{"elapsed":428899,"status":"error","timestamp":1714886829229,"user":{"displayName":"Chika Maduabuchi","userId":"13808453024378790419"},"user_tz":240},"id":"JYp-m8W0ijKW","outputId":"c29d6173-54b4-4b83-d241-50ea35a10f87"},"outputs":[],"source":["import os\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from transformers import SamProcessor, SamModel\n","from tqdm.notebook import tqdm\n","from statistics import mean\n","import monai\n","from transformers import AdamW\n","from peft import LoraConfig, get_peft_model\n","import logging\n","import time\n","\n","class SAMDataset(Dataset):\n","    def __init__(self, dataset, processor):\n","        self.dataset = dataset\n","        self.processor = processor\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        item = self.dataset[idx]\n","        image = np.array(item[\"image\"])\n","        ground_truth_mask = np.array(item[\"label\"])\n","        image = np.repeat(image.reshape(image.shape[0], image.shape[1], 1), 3, axis=-1) if len(image.shape) == 2 or image.shape[-1] == 1 else image\n","        prompt = self.get_bounding_box(ground_truth_mask)\n","        inputs = self.processor(image, input_boxes=[[prompt]], return_tensors=\"pt\", do_rescale=False)\n","        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n","        inputs[\"ground_truth_mask\"] = torch.tensor(ground_truth_mask, dtype=torch.float32).unsqueeze(0)\n","        return inputs\n","\n","    def get_bounding_box(self, ground_truth_map):\n","        y_indices, x_indices = np.where(np.squeeze(ground_truth_map) > 0)\n","        x_min, x_max = np.min(x_indices), np.max(x_indices)\n","        y_min, y_max = np.min(y_indices), np.max(y_indices)\n","        H, W = np.squeeze(ground_truth_map).shape\n","        x_min, x_max = max(0, x_min - np.random.randint(0, 20)), min(W, x_max + np.random.randint(0, 20))\n","        y_min, y_max = max(0, y_min - np.random.randint(0, 20)), min(H, y_max + np.random.randint(0, 20))\n","        return [x_min, y_min, x_max, y_max]\n","\n","def load_model(model_name):\n","    model = SamModel.from_pretrained(model_name)\n","    for name, param in model.named_parameters():\n","        if name.startswith(\"vision_encoder\") or name.startswith(\"prompt_encoder\"):\n","            param.requires_grad_(False)\n","    return model\n","\n","def train(model, train_loader, val_loader, optimizer, seg_loss, device, num_epochs, logger, strategy_name, early_stop_patience=10, start_epoch=0):\n","    losses = []\n","    val_losses = []\n","    best_val_loss = float('inf')\n","    epochs_no_improve = 0\n","\n","    for epoch in range(start_epoch, num_epochs):\n","        epoch_start_time = time.time()  # Track start time\n","        model.train()\n","        epoch_losses = []\n","        progress_bar = tqdm(train_loader, desc=f\"Training [{strategy_name}] Epoch {epoch + 1}/{num_epochs}\")\n","\n","        for batch_idx, batch in enumerate(progress_bar, 1):\n","            outputs = model(pixel_values=batch[\"pixel_values\"].to(device), input_boxes=batch[\"input_boxes\"].to(device), multimask_output=False)\n","            loss = seg_loss(outputs.pred_masks.squeeze(1), batch[\"ground_truth_mask\"].float().to(device))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_losses.append(loss.item())\n","            progress_bar.set_postfix({'Batch Loss': loss.item()})\n","\n","        mean_loss = mean(epoch_losses)\n","        losses.append(mean_loss)\n","        epoch_end_time = time.time()  # Track end time\n","        epoch_duration = epoch_end_time - epoch_start_time\n","        logger.info(f'Strategy: {strategy_name} | EPOCH: {epoch + 1}/{num_epochs}, Mean Loss: {mean_loss}, Duration: {epoch_duration:.2f}s')\n","\n","        val_loss = validate(model, val_loader, seg_loss, device, strategy_name, epoch, num_epochs)\n","        val_losses.append(val_loss)\n","        logger.info(f'Strategy: {strategy_name} | EPOCH: {epoch + 1}/{num_epochs}, Validation Loss: {val_loss}')\n","\n","        # Save the best model\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            epochs_no_improve = 0\n","            best_model_path = f\"/home/ubuntu/models/{strategy_name}_best.pth\"\n","            torch.save(model.state_dict(), best_model_path)\n","            logger.info(f\"Best model saved at {best_model_path}\")\n","        else:\n","            epochs_no_improve += 1\n","\n","        # Save checkpoint at the end of each epoch, overwriting the previous one\n","        checkpoint_path = f\"/home/ubuntu/models/{strategy_name}_checkpoint.pth\"\n","        torch.save({\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'epoch': epoch + 1,  # Save the next epoch number\n","            'best_val_loss': best_val_loss,\n","            'train_losses': losses,\n","            'val_losses': val_losses\n","        }, checkpoint_path)\n","        logger.info(f\"Checkpoint saved at {checkpoint_path}\")\n","\n","        # Early stopping\n","        if epochs_no_improve >= early_stop_patience:\n","            logger.info(f\"Early stopping at epoch {epoch + 1} due to no improvement in validation loss for {early_stop_patience} consecutive epochs\")\n","            break\n","\n","    # Save the final model\n","    final_model_path = f\"/home/ubuntu/models/{strategy_name}_final.pth\"\n","    torch.save(model.state_dict(), final_model_path)\n","    logger.info(f\"Final model saved at {final_model_path}\")\n","\n","    return losses, val_losses\n","\n","def validate(model, dataloader, seg_loss, device, strategy_name, epoch, num_epochs):\n","    model.eval()\n","    val_losses = []\n","    with torch.no_grad():\n","        progress_bar = tqdm(dataloader, desc=f\"Validating [{strategy_name}] Epoch {epoch + 1}/{num_epochs}\")\n","        for batch in progress_bar:\n","            outputs = model(pixel_values=batch[\"pixel_values\"].to(device), input_boxes=batch[\"input_boxes\"].to(device), multimask_output=False)\n","            loss = seg_loss(outputs.pred_masks.squeeze(1), batch[\"ground_truth_mask\"].float().to(device))\n","            val_losses.append(loss.item())\n","            progress_bar.set_postfix({'Batch Loss': loss.item()})\n","    mean_val_loss = mean(val_losses)\n","    return mean_val_loss\n","\n","def plot_all_losses(all_losses):\n","    plt.figure(figsize=(10, 5))\n","    for strategy_name, (train_losses, val_losses) in all_losses.items():\n","        plt.plot(train_losses, marker='o', linestyle='-', label=f'{strategy_name} Train')\n","        plt.plot(val_losses, marker='x', linestyle='--', label=f'{strategy_name} Val')\n","    plt.title('Loss Evolution Across Epochs for All Strategies', fontsize=14)\n","    plt.xlabel('Epochs', fontsize=12)\n","    plt.ylabel('Loss', fontsize=12)\n","    plt.grid(True)\n","    plt.legend(fontsize=12)\n","    plt.savefig('/home/ubuntu/plots/loss_evolution_all_strategies.jpg', dpi=300)\n","    plt.show()\n","\n","def apply_partial_finetuning(model):\n","    for name, param in model.named_parameters():\n","        param.requires_grad = name.startswith(\"mask_decoder\")\n","    return model\n","\n","def apply_lora(model, config):\n","    return get_peft_model(model, config)\n","\n","def apply_adapter_tuning(model):\n","    from transformers.adapters import AdapterConfig\n","    config = AdapterConfig()\n","    model.add_adapter(\"adapter\", config=config)\n","    model.train_adapter(\"adapter\")\n","    return model\n","\n","def apply_prefix_tuning(model):\n","    from transformers import PrefixTuningConfig, PrefixTuning\n","    config = PrefixTuningConfig()\n","    prefix_tuning = PrefixTuning(config=config)\n","    model.add_prefix_tuning(prefix_tuning)\n","    return model\n","\n","def apply_bitfit(model):\n","    for name, param in model.named_parameters():\n","        param.requires_grad = 'bias' in name\n","    return model\n","\n","def apply_prompt_tuning(model):\n","    from transformers import PromptTuningConfig, PromptTuning\n","    config = PromptTuningConfig()\n","    prompt_tuning = PromptTuning(config=config)\n","    model.add_prompt_tuning(prompt_tuning)\n","    return model\n","\n","def apply_layerwise_lr_decay(optimizer, model, base_lr, lr_decay):\n","    params = [{\"params\": param, \"lr\": base_lr * (lr_decay ** int(name.split(\".\")[1])) if \"layer\" in name else base_lr} for name, param in model.named_parameters() if param.requires_grad]\n","    return optimizer.__class__(params, lr=base_lr)\n","\n","def apply_differential_learning_rates(model, base_lr, task_lr):\n","    params = [{\"params\": model.mask_decoder.parameters(), \"lr\": task_lr},\n","              {\"params\": model.vision_encoder.parameters(), \"lr\": base_lr * 0.1},\n","              {\"params\": model.prompt_encoder.parameters(), \"lr\": base_lr * 0.01}]\n","    return AdamW(params, lr=base_lr)\n","\n","def apply_gradual_unfreezing(model):\n","    for param in model.parameters():\n","        param.requires_grad = False\n","    for name, param in model.named_parameters():\n","        if \"mask_decoder\" in name:\n","            param.requires_grad = True\n","    return model\n","\n","def apply_knowledge_distillation(teacher_model, student_model, dataloader, device, num_epochs, logger, strategy_name):\n","    criterion = torch.nn.KLDivLoss(reduction=\"batchmean\")\n","    optimizer = AdamW(student_model.parameters(), lr=1e-5)\n","    student_model.train()\n","    teacher_model.eval()\n","    losses = []\n","    for epoch in range(num_epochs):\n","        epoch_losses = []\n","        progress_bar = tqdm(dataloader, desc=f\"Knowledge Distillation [{strategy_name}] Epoch {epoch + 1}/{num_epochs}\")\n","        for batch_idx, batch in enumerate(progress_bar, 1):\n","            student_outputs = student_model(pixel_values=batch[\"pixel_values\"].to(device), input_boxes=batch[\"input_boxes\"].to(device), multimask_output=False)\n","            with torch.no_grad():\n","                teacher_outputs = teacher_model(pixel_values=batch[\"pixel_values\"].to(device), input_boxes=batch[\"input_boxes\"].to(device), multimask_output=False)\n","            loss = criterion(student_outputs.pred_masks.squeeze(1).log_softmax(dim=-1), teacher_outputs.pred_masks.squeeze(1).softmax(dim=-1))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            epoch_losses.append(loss.item())\n","            progress_bar.set_postfix({'Batch Loss': loss.item()})\n","        mean_loss = mean(epoch_losses)\n","        losses.append(mean_loss)\n","        logger.info(f'Strategy: {strategy_name} | EPOCH: {epoch + 1}/{num_epochs}, Mean Loss: {mean_loss}')\n","    return losses\n","\n","def load_checkpoint(model, optimizer, checkpoint_path):\n","    if os.path.isfile(checkpoint_path):\n","        checkpoint = torch.load(checkpoint_path)\n","        print(f\"Checkpoint keys: {checkpoint.keys()}\")  # Debug print\n","        model.load_state_dict(checkpoint['model_state_dict'])\n","        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","        start_epoch = checkpoint.get('epoch', 0)\n","        best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n","        train_losses = checkpoint.get('train_losses', [])\n","        val_losses = checkpoint.get('val_losses', [])\n","        return model, optimizer, start_epoch, best_val_loss, train_losses, val_losses\n","    else:\n","        return model, optimizer, 0, float('inf'), [], []\n","\n","def main():\n","    logging.basicConfig(filename='/home/ubuntu/train.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","    logger = logging.getLogger()\n","\n","    os.makedirs('/home/ubuntu/plots', exist_ok=True)\n","    os.makedirs('/home/ubuntu/models', exist_ok=True)\n","\n","    processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n","\n","    # Ensure the dataset variable is correctly loaded\n","    # Example: dataset = load_your_dataset_function()\n","\n","    train_dataset = SAMDataset(dataset=dataset, processor=processor)\n","    train_length = int(0.8 * len(train_dataset))\n","    val_length = len(train_dataset) - train_length\n","    train_dataset, val_dataset = random_split(train_dataset, [train_length, val_length])\n","    train_dataloader = DataLoader(train_dataset, batch_size=2, shuffle=True, drop_last=False)\n","    val_dataloader = DataLoader(val_dataset, batch_size=2, shuffle=False, drop_last=False)\n","\n","    # Define the LoRA configuration with appropriate target modules\n","    lora_config = LoraConfig(\n","        r=16,\n","        lora_alpha=32,\n","        lora_dropout=0.1,\n","        target_modules=[\n","            \"vision_encoder.layers.0.attn.qkv\",\n","            \"vision_encoder.layers.0.attn.proj\",\n","            \"vision_encoder.layers.0.mlp.lin1\",\n","            \"vision_encoder.layers.0.mlp.lin2\",\n","            \"mask_decoder.transformer.layers.0.self_attn.q_proj\",\n","            \"mask_decoder.transformer.layers.0.self_attn.k_proj\",\n","            \"mask_decoder.transformer.layers.0.self_attn.v_proj\",\n","            \"mask_decoder.transformer.layers.0.self_attn.out_proj\",\n","            \"mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj\",\n","            \"mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj\",\n","            \"mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj\",\n","            \"mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj\",\n","            \"mask_decoder.transformer.layers.0.mlp.lin1\",\n","            \"mask_decoder.transformer.layers.0.mlp.lin2\",\n","            \"mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj\",\n","            \"mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj\",\n","            \"mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj\",\n","            \"mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj\"\n","        ]\n","    )\n","\n","    strategies = {\n","        \"lora\": lambda model: apply_lora(model, lora_config),\n","        \"adapter\": apply_adapter_tuning,\n","        \"prefix_tuning\": apply_prefix_tuning,\n","        \"bitfit\": apply_bitfit,\n","        \"prompt_tuning\": apply_prompt_tuning,\n","        \"layerwise_lr_decay\": lambda model: model,\n","        \"differential_lr\": lambda model: model,\n","        \"gradual_unfreezing\": apply_gradual_unfreezing,\n","        \"knowledge_distillation\": lambda model: model\n","    }\n","\n","    num_epochs = 100\n","    seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","    teacher_model = load_model(\"facebook/sam-vit-large\")\n","\n","    all_losses = {}\n","    total_strategies = len(strategies)\n","\n","    for i, (strategy_name, strategy_function) in enumerate(strategies.items(), 1):\n","        logger.info(f\"Starting strategy {i}/{total_strategies}: {strategy_name}\")\n","        model = load_model(\"facebook/sam-vit-base\")\n","        model = strategy_function(model)\n","        model.to(device)\n","\n","        if strategy_name == \"layerwise_lr_decay\":\n","            optimizer = apply_layerwise_lr_decay(AdamW, model, base_lr=1e-5, lr_decay=0.95)\n","        elif strategy_name == \"differential_lr\":\n","            optimizer = apply_differential_learning_rates(model, base_lr=1e-5, task_lr=1e-4)\n","        else:\n","            optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0)\n","\n","        # Load checkpoint if it exists\n","        checkpoint_path = f\"/home/ubuntu/models/{strategy_name}_checkpoint.pth\"\n","        start_epoch = 0  # Default start epoch\n","        if os.path.isfile(checkpoint_path):\n","            print(f\"Loading checkpoint from {checkpoint_path}\")  # Debug print\n","            model, optimizer, start_epoch, best_val_loss, train_losses, val_losses = load_checkpoint(model, optimizer, checkpoint_path)\n","            logger.info(f\"Resuming training from epoch {start_epoch} for strategy {strategy_name}\")\n","        else:\n","            best_val_loss = float('inf')\n","            train_losses = []\n","            val_losses = []\n","\n","        if strategy_name == \"knowledge_distillation\":\n","            losses = apply_knowledge_distillation(teacher_model, model, train_dataloader, device, num_epochs, logger, strategy_name)\n","            all_losses[strategy_name] = (losses, [None] * len(losses))\n","        else:\n","            train_losses, val_losses = train(model, train_dataloader, val_dataloader, optimizer, seg_loss, device, num_epochs, logger, strategy_name, early_stop_patience=10, start_epoch=start_epoch)\n","            all_losses[strategy_name] = (train_losses, val_losses)\n","\n","        model_save_path = f'/home/ubuntu/models/{strategy_name}_model.pth'\n","        torch.save(model.state_dict(), model_save_path)\n","        logger.info(f\"Model saved at {model_save_path}\")\n","        logger.info(f\"Completed strategy {i}/{total_strategies}: {strategy_name}\")\n","\n","    plot_all_losses(all_losses)\n","\n","if __name__ == \"__main__\":\n","    main()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# 6. Data Analysis"]},{"cell_type":"markdown","metadata":{},"source":["## 6.1 Training/Validation Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import re\n","import os\n","\n","# Provided values for epoch 1\n","EPOCH_1_TRAIN_LOSS = 0.29\n","EPOCH_1_VAL_LOSS = 0.15\n","\n","def extract_loss_metrics(log_lines):\n","    \"\"\"Extracts loss metrics from the given log lines.\"\"\"\n","    metrics = []\n","    for line in log_lines:\n","        if \"Val Loss\" in line:\n","            epoch = int(re.search(r'EPOCH: (\\d+)/\\d+', line).group(1))\n","            val_loss = float(re.search(r'Val Loss: ([\\d.]+)', line).group(1))\n","            metrics.append([epoch, val_loss, 'Val Loss'])\n","        elif \"Mean Loss\" in line:\n","            epoch = int(re.search(r'EPOCH: (\\d+)/\\d+', line).group(1))\n","            train_loss = float(re.search(r'Mean Loss: ([\\d.]+)', line).group(1))\n","            metrics.append([epoch, train_loss, 'Train Loss'])\n","    return metrics\n","\n","def read_log_file(file_path):\n","    \"\"\"Reads the content of the log file.\"\"\"\n","    with open(file_path, 'r') as file:\n","        return file.readlines()\n","\n","def combine_loss_metrics(log_paths):\n","    \"\"\"Combines loss metrics from multiple log files.\"\"\"\n","    all_metrics = []\n","    for path in log_paths:\n","        log_lines = read_log_file(path)\n","        all_metrics.extend(extract_loss_metrics(log_lines))\n","    return all_metrics\n","\n","def create_loss_dataframe(metrics):\n","    \"\"\"Creates DataFrame from the loss metrics.\"\"\"\n","    df_loss = pd.DataFrame(metrics, columns=[\"Epoch\", \"Loss\", \"Type\"])\n","    df_train_loss = df_loss[df_loss[\"Type\"] == \"Train Loss\"].drop_duplicates(subset=\"Epoch\").sort_values(by=\"Epoch\")\n","    df_val_loss = df_loss[df_loss[\"Type\"] == \"Val Loss\"].drop_duplicates(subset=\"Epoch\").sort_values(by=\"Epoch\")\n","    \n","    # Manually add the provided values for epoch 1\n","    df_train_loss = pd.concat([pd.DataFrame([[1, EPOCH_1_TRAIN_LOSS, 'Train Loss']], columns=df_train_loss.columns), df_train_loss], ignore_index=True)\n","    df_val_loss = pd.concat([pd.DataFrame([[1, EPOCH_1_VAL_LOSS, 'Val Loss']], columns=df_val_loss.columns), df_val_loss], ignore_index=True)\n","\n","    # Ensure the epochs range from 1 to 22\n","    epochs_range = pd.DataFrame({'Epoch': list(range(1, 23))})\n","\n","    # Merge DataFrames to ensure consistent epochs\n","    df_combined = pd.merge(epochs_range, df_train_loss, on=\"Epoch\", how=\"left\").merge(df_val_loss, on=\"Epoch\", how=\"left\", suffixes=('_train', '_val'))\n","    return df_combined\n","\n","def plot_metrics(df_combined, epochs, iou, precision_epochs, recall_epochs):\n","    \"\"\"Plots the training/validation loss and validation metrics.\"\"\"\n","    fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n","    \n","    # Customization parameters\n","    title_fontsize = 18\n","    label_fontsize = 16\n","    tick_fontsize = 14\n","    legend_fontsize = 14\n","    line_width = 2.5\n","    marker_size = 10\n","\n","    # Training/Validation Loss plot\n","    axes[0].plot(df_combined['Epoch'], df_combined['Loss_train'], marker='o', linestyle='-', color='b', label='Training Loss', linewidth=line_width, markersize=marker_size)\n","    axes[0].plot(df_combined['Epoch'], df_combined['Loss_val'], marker='x', linestyle='--', color='r', label='Validation Loss', linewidth=line_width, markersize=marker_size)\n","    axes[0].set_title('Loss Evolution Across Epochs', fontsize=title_fontsize)\n","    axes[0].set_xlabel('Epochs', fontsize=label_fontsize)\n","    axes[0].set_ylabel('Loss', fontsize=label_fontsize)\n","    axes[0].legend(fontsize=legend_fontsize)\n","    axes[0].grid(which='both', linestyle='--', linewidth=0.5)\n","    axes[0].tick_params(axis='both', which='major', labelsize=tick_fontsize)\n","    axes[0].tick_params(axis='both', which='minor', labelsize=tick_fontsize)\n","    axes[0].minorticks_on()\n","    axes[0].spines['top'].set_linewidth(2)\n","    axes[0].spines['right'].set_linewidth(2)\n","    axes[0].spines['bottom'].set_linewidth(2)\n","    axes[0].spines['left'].set_linewidth(2)\n","\n","    # Validation Metrics plot\n","    axes[1].plot(epochs, iou, marker='o', linestyle='-', color='b', label='IoU', linewidth=line_width, markersize=marker_size)\n","    axes[1].plot(epochs, precision_epochs, marker='x', linestyle='--', color='r', label='Precision', linewidth=line_width, markersize=marker_size)\n","    axes[1].plot(epochs, recall_epochs, marker='s', linestyle='-.', color='g', label='Recall', linewidth=line_width, markersize=marker_size)\n","    axes[1].set_title('Validation Metrics Across Epochs', fontsize=title_fontsize)\n","    axes[1].set_xlabel('Epochs', fontsize=label_fontsize)\n","    axes[1].set_ylabel('Metrics', fontsize=label_fontsize)\n","    axes[1].legend(fontsize=legend_fontsize)\n","    axes[1].grid(which='both', linestyle='--', linewidth=0.5)\n","    axes[1].tick_params(axis='both', which='major', labelsize=tick_fontsize)\n","    axes[1].tick_params(axis='both', which='minor', labelsize=tick_fontsize)\n","    axes[1].minorticks_on()\n","    axes[1].spines['top'].set_linewidth(2)\n","    axes[1].spines['right'].set_linewidth(2)\n","    axes[1].spines['bottom'].set_linewidth(2)\n","    axes[1].spines['left'].set_linewidth(2)\n","\n","    plt.tight_layout()\n","\n","    # Save the combined plot\n","    if not os.path.exists('plots/training'):\n","        os.makedirs('plots/training')\n","    plt.savefig('plots/training/training_validation_metrics_combined.jpg', dpi=300)\n","\n","    plt.show()\n","\n","# File paths\n","log_paths = ['train copy.log', 'train.log']\n","\n","# Combine and process loss metrics\n","all_loss_metrics = combine_loss_metrics(log_paths)\n","df_loss_combined = create_loss_dataframe(all_loss_metrics)\n","\n","# Data for validation metrics\n","epochs = list(range(1, 23))\n","iou = [0.91, 0.914707, 0.916660, 0.918699, 0.913917, 0.930954, 0.933003, 0.933262, 0.934331, 0.930741, 0.936661, 0.936258, 0.936562, 0.940284, 0.939111, 0.939553, 0.936558, 0.942341, 0.941713, 0.940420, 0.942994, 0.934319]\n","precision_epochs = [0.95, 0.975052, 0.976476, 0.957124, 0.983308, 0.972965, 0.975878, 0.978484, 0.978311, 0.982931, 0.972256, 0.966213, 0.981463, 0.976788, 0.980571, 0.980384, 0.983494, 0.977174, 0.979757, 0.982874, 0.974158, 0.985832]\n","recall_epochs = [0.90, 0.936321, 0.937073, 0.957338, 0.928094, 0.955297, 0.954742, 0.952513, 0.953805, 0.945748, 0.962018, 0.967640, 0.953185, 0.961521, 0.956682, 0.957330, 0.951369, 0.963307, 0.960226, 0.955912, 0.966943, 0.946898]\n","\n","# Plot the metrics\n","plot_metrics(df_loss_combined, epochs, iou, precision_epochs, recall_epochs)"]},{"cell_type":"markdown","metadata":{},"source":["## 6.3 Inference Results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Load the provided CSV file\n","file_path = 'plots/single_tiff/metrics_comparison_table.csv'\n","data = pd.read_csv(file_path)\n","\n","# Define the directory and create it if it doesn't exist\n","output_dir = 'plots/single_tiff'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Define the metrics to visualize (only F1 Score, IoU, and Precision)\n","metrics = [\"Precision\", \"F1 Score\", \"IoU\"]\n","\n","# Improved bar charts for metric comparison with scientifically appropriate colors\n","fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\n","\n","# Scientifically appropriate color palette\n","palette = {\"VideoSAM\": \"blue\", \"SAM\": \"green\"}\n","\n","for i, metric in enumerate(metrics):\n","    sns.barplot(ax=axes[i], data=data, x='Fluid', y=metric, hue='Model', palette=palette)\n","    axes[i].set_title(f'Comparison of {metric}', fontsize=18, weight='bold')\n","    axes[i].set_xlabel('Fluid', fontsize=16)\n","    axes[i].set_ylabel(metric, fontsize=16)\n","    axes[i].legend(title='Model', fontsize=14)\n","    axes[i].tick_params(axis='both', which='major', labelsize=14)\n","    axes[i].tick_params(axis='both', which='minor', labelsize=12)\n","    axes[i].grid(True, linestyle='--', linewidth=0.5)\n","    axes[i].spines['top'].set_linewidth(2)\n","    axes[i].spines['right'].set_linewidth(2)\n","    axes[i].spines['bottom'].set_linewidth(2)\n","    axes[i].spines['left'].set_linewidth(2)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(output_dir, 'bar_charts_comparison.jpg'), dpi=300)\n","plt.show()\n","\n","# Corrected heatmap\n","heatmap_data = data.melt(id_vars=[\"Fluid\", \"Model\"], var_name=\"Metric\", value_name=\"Value\")\n","heatmap_data = heatmap_data[heatmap_data['Metric'].isin(metrics)]\n","heatmap_pivot = heatmap_data.pivot_table(index=[\"Fluid\", \"Metric\"], columns=\"Model\", values=\"Value\")\n","heatmap_pivot = heatmap_pivot.unstack(level='Metric')\n","\n","plt.figure(figsize=(14, 8))\n","sns.heatmap(heatmap_pivot, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=.5, cbar_kws={'label': 'Value'})\n","plt.title('Heatmap of Model Performance Across Metrics', fontsize=18, weight='bold')\n","plt.tick_params(axis='both', which='major', labelsize=14)\n","plt.tick_params(axis='both', which='minor', labelsize=12)\n","plt.tight_layout()\n","plt.savefig(os.path.join(output_dir, 'heatmap_comparison.jpg'), dpi=300)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Ensure the directory exists\n","output_dir = 'plots/composite_tiff'\n","os.makedirs(output_dir, exist_ok=True)\n","\n","# Load the dataset\n","file_path = os.path.join(output_dir, 'metrics_comparison_summary_table.csv')\n","data = pd.read_csv(file_path)\n","\n","# Prepare data for box plot\n","def prepare_box_plot_data(metric_mean, metric_std):\n","    return {\n","        'Lower Whisker': metric_mean - metric_std,\n","        'Lower Quartile': metric_mean - 0.5 * metric_std,\n","        'Median': metric_mean,\n","        'Upper Quartile': metric_mean + 0.5 * metric_std,\n","        'Upper Whisker': metric_mean + metric_std\n","    }\n","\n","# Collect data for each dataset\n","datasets = data['Fluid']\n","metrics = ['VideoSAM_IoU_Mean', 'VideoSAM_F1 Score_Mean', 'SAM_IoU_Mean', 'SAM_F1 Score_Mean']\n","std_devs = ['VideoSAM_IoU_StdDev', 'VideoSAM_F1 Score_StdDev', 'SAM_IoU_StdDev', 'SAM_F1 Score_StdDev']\n","\n","# Initialize a dictionary to store the box plot data\n","box_plot_data = {metric: {dataset: {} for dataset in datasets} for metric in metrics}\n","\n","# Populate the dictionary with the prepared box plot data\n","for metric, std_dev in zip(metrics, std_devs):\n","    for idx, dataset in enumerate(datasets):\n","        box_plot_data[metric][dataset] = prepare_box_plot_data(data[metric][idx], data[std_dev][idx])\n","\n","# Define plot style\n","plt.style.use('ggplot')\n","plt.rcParams.update({\n","    \"figure.figsize\": (18, 14),\n","    \"axes.titlesize\": 20,\n","    \"axes.labelsize\": 18,\n","    \"xtick.labelsize\": 16,\n","    \"ytick.labelsize\": 16,\n","    \"legend.fontsize\": 16,\n","    \"font.family\": \"serif\"\n","})\n","\n","# Custom color palette for better differentiation\n","colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728']\n","\n","# Create box plots for each metric\n","fig, axes = plt.subplots(2, 2, sharey=True)\n","fig.subplots_adjust(hspace=0.4, wspace=0.4)\n","\n","for ax, (metric, color) in zip(axes.flatten(), zip(metrics, colors)):\n","    # Extract the box plot values for each dataset\n","    box_values = [list(box_plot_data[metric][dataset].values()) for dataset in datasets]\n","    \n","    # Plot the box plot\n","    ax.boxplot(box_values, labels=datasets, patch_artist=True,\n","               boxprops=dict(facecolor=color, color='black'),\n","               medianprops=dict(color='red', linewidth=2),\n","               whiskerprops=dict(color='black', linewidth=2),\n","               capprops=dict(color='black', linewidth=2),\n","               flierprops=dict(marker='o', color='black', alpha=0.5))\n","    \n","    # Update title and ylabel\n","    metric_name = 'IoU' if 'IoU' in metric else 'F1-Score'\n","    ax.set_title(f'{metric.split(\"_\")[0]} {metric_name} by Dataset', fontsize=20, weight='bold')\n","    ax.set_ylabel(metric_name, fontsize=18)\n","    ax.grid(True, linestyle='--', linewidth=0.7)\n","    \n","    # Enhance the border of the plot\n","    for spine in ax.spines.values():\n","        spine.set_linewidth(2)\n","\n","# Adjust layout for better readability\n","plt.tight_layout()\n","\n","# Save the plot as a 300 dpi JPEG file\n","output_path = os.path.join(output_dir, 'composite_frame_analysis.jpg')\n","plt.savefig(output_path, format='jpg', dpi=300)\n","\n","# Show plot\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["## 6.4 Validation Analysis with U-Net CNN "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import tifffile as tiff\n","import h5py\n","from sklearn.metrics import f1_score, jaccard_score\n","from tqdm import tqdm\n","from skimage.transform import resize\n","\n","def normalize_unet_masks(masks):\n","    return (masks > 0).astype(np.float32)\n","\n","# Load the U-Net mask and ground truth files\n","unet_mask_path = '/Users/chikamaduabuchi/Library/CloudStorage/GoogleDrive-sparkresearchers@gmail.com/My Drive/videosam/archived/u-net/masks/iter_50.tif'\n","gt_path = '/Users/chikamaduabuchi/Library/CloudStorage/GoogleDrive-sparkresearchers@gmail.com/My Drive/videosam/archived/data/test/test_mask.tif'\n","\n","print(\"Loading U-Net mask and ground truth files...\")\n","unet_mask = tiff.imread(unet_mask_path)\n","gt = tiff.imread(gt_path)\n","\n","# Resize U-Net mask frames to match the ground truth frame size (256x256)\n","print(\"Resizing U-Net mask frames to 256x256...\")\n","unet_mask_resized = np.zeros((unet_mask.shape[0], 256, 256), dtype=np.uint8)\n","\n","for i in range(unet_mask.shape[0]):\n","    unet_mask_resized[i] = resize(unet_mask[i], (256, 256), order=0, preserve_range=True).astype(np.uint8)\n","\n","# Normalize the U-Net masks and the ground truth\n","unet_mask_resized = normalize_unet_masks(unet_mask_resized)\n","gt = normalize_unet_masks(gt)\n","\n","# Define the frame ranges for each fluid\n","frame_ranges = {\n","    'water': range(0, 250),\n","    'fc-72': range(250, 500),\n","    'nitrogen': range(500, 750),\n","    'argon': range(750, 1000)\n","}\n","\n","# Initialize lists to store IoU and F1 scores for each model and fluid\n","metrics = ['IoU', 'F1 Score']\n","results = {fluid: {model: {metric: [] for metric in metrics} for model in ['U-Net']} for fluid in frame_ranges}\n","visuals = []\n","\n","# Process the masks\n","for fluid, frames in frame_ranges.items():\n","    for i in tqdm(frames, desc=f\"Processing {fluid} frames\"):\n","        # Flatten the ground truth and U-Net masks for scoring\n","        gt_frame = gt[i].flatten()\n","        unet_mask_frame = unet_mask_resized[i].flatten()\n","        \n","        # Calculate IoU and F1 scores for U-Net\n","        if np.sum(gt_frame) > 0 and np.sum(unet_mask_frame) > 0:\n","            results[fluid]['U-Net']['IoU'].append(jaccard_score(gt_frame, unet_mask_frame, average='binary'))\n","            results[fluid]['U-Net']['F1 Score'].append(f1_score(gt_frame, unet_mask_frame, average='binary'))\n","\n","        # Store visuals for summary\n","        if i % 250 == 0:\n","            visuals.append((gt[i], unet_mask_resized[i], i, fluid))\n","\n","# Calculate mean IoU and F1 score for each model and fluid\n","mean_results = {fluid: {model: {metric: np.mean(results[fluid][model][metric]) if results[fluid][model][metric] else 0 for metric in metrics} for model in ['U-Net']} for fluid in frame_ranges}\n","\n","# Output the results\n","print(\"Mean IoU and F1 Score per fluid and model:\")\n","for fluid in frame_ranges:\n","    for model in ['U-Net']:\n","        print(f\"{fluid} - {model}: Mean IoU = {mean_results[fluid][model]['IoU']:.4f}, Mean F1 = {mean_results[fluid][model]['F1 Score']:.4f}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def visualize_unet_vs_gt(visuals):\n","    fig, axs = plt.subplots(len(visuals), 2, figsize=(15, 5 * len(visuals)))\n","\n","    for i, (gt, unet_mask, frame_index, fluid) in enumerate(visuals):\n","        # Ground Truth\n","        axs[i, 0].imshow(gt, cmap='gray')\n","        axs[i, 0].set_title(f'Ground Truth (Frame {frame_index})', fontsize=14, weight='bold')\n","        axs[i, 0].set_xlabel(f'{fluid.capitalize()}', fontsize=12)\n","        axs[i, 0].axis('off')\n","        \n","        # U-Net Mask\n","        axs[i, 1].imshow(unet_mask, cmap='gray')\n","        axs[i, 1].set_title(f'U-Net Mask (Frame {frame_index})', fontsize=14, weight='bold')\n","        axs[i, 1].set_xlabel(f'{fluid.capitalize()}', fontsize=12)\n","        axs[i, 1].axis('off')\n","    \n","    # Adjust layout to ensure titles and labels fit nicely\n","    plt.tight_layout(pad=2)\n","    plt.show()\n","\n","# Visualize the collected masks\n","visualize_unet_vs_gt(visuals)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["!pip install tabulate"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from tabulate import tabulate\n","\n","# Data from the results you provided\n","fluids = ['Water', 'FC-72', 'Nitrogen', 'Argon']\n","\n","# U-Net Results\n","unet_iou = [0.5619, 0.7244, 0.7547, 0.7815]\n","unet_f1 = [0.7191, 0.8400, 0.8602, 0.8773]\n","\n","# VideoSAM Results\n","videosam_iou = [0.189444, 0.799655, 0.831651, 0.838407]\n","videosam_f1 = [0.314252, 0.888476, 0.908027, 0.912041]\n","\n","# Create a DataFrame for the results\n","data = {\n","    'Fluid': fluids,\n","    'U-Net Mean IoU': unet_iou,\n","    'VideoSAM Mean IoU': videosam_iou,\n","    'U-Net Mean F1 Score': unet_f1,\n","    'VideoSAM Mean F1 Score': videosam_f1\n","}\n","\n","df = pd.DataFrame(data)\n","\n","# Display the table using tabulate for better formatting\n","table = tabulate(df, headers='keys', tablefmt='pipe', showindex=False)\n","print(table)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
